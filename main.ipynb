{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "import IPython.display as ipd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, Subset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision.utils import make_grid  # |   Utility stuff for plotting\n",
    "import matplotlib.pyplot as plt          # |  <- I use this one a lot for plotting, seaborn is a good alternative\n",
    "from matplotlib.image import imread      # |  it reads images... (png -> usable input (like a numpy array for ex))\n",
    "import os\n",
    "import random\n",
    "from tqdm import tqdm  # | This one is a cute one for making a loading bar, I like it and we'll use it here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(directory, batch_size, train_ratio=0.8):\n",
    "    dataset = []\n",
    "    emotion_to_number = {'NEU': 0, 'HAP': 1, 'SAD': 2, 'ANG': 3, 'DIS': 4, 'FEA': 5}\n",
    "    max_len = 0\n",
    "    \n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.wav'):\n",
    "            filepath = os.path.join(directory, filename)\n",
    "            emotion = filename.split('_')[2]\n",
    "            emotion_no = emotion_to_number[emotion]\n",
    "            emotion_label = torch.zeros(6)\n",
    "            emotion_label[emotion_no] = 1\n",
    "            audio_tensor, _ = librosa.load(filepath, sr=None)\n",
    "            max_len = max(max_len, len(audio_tensor))\n",
    "            audio_tensor = torch.tensor(audio_tensor, dtype=torch.float32)\n",
    "            dataset.append([audio_tensor, emotion_label])\n",
    "\n",
    "    dataset = [(torch.nn.functional.pad(audio_tensor, (0, max_len - audio_tensor.size(0))), label)\n",
    "               for audio_tensor, label in dataset]\n",
    "\n",
    "    # Split the dataset into training and testing sets\n",
    "    train_size = int(train_ratio * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "    train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "    # Create DataLoader objects for training and testing sets\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return train_dataset, test_dataset, train_dataloader, test_dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0008, -0.0003, -0.0006,  ...,  0.0000,  0.0000,  0.0000])\n",
      "tensor([0., 0., 0., 0., 1., 0.])\n"
     ]
    }
   ],
   "source": [
    "crema_d_directory = \"./AudioWAV\"\n",
    "train_dataset, test_dataset, train_dataloader, test_dataloader = load_dataset(crema_d_directory, batch_size=32)\n",
    "ex_audio, ex_label = train_dataset[random.randint(0,20)]\n",
    "print(ex_audio)\n",
    "print(ex_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AudioCNN(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(AudioCNN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=32, kernel_size=3)\n",
    "        self.conv2 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3)\n",
    "        \n",
    "        # Calculate the output size of the convolutional layers\n",
    "        conv_output_size = (((input_size-2)//2) - 2)//2  # Considering two max pooling layers with kernel_size=2\n",
    "        \n",
    "        self.fc1 = nn.Linear(64 * conv_output_size, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool1d(x, kernel_size=2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool1d(x, kernel_size=2)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000, 0.0388, 0.0000, 0.0000, 0.0000, 0.1050]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([0., 1., 0., 0., 0., 0.])\n",
      "tensor([0, 0, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_size = 80080\n",
    "num_classes = 6  # 6 emotion classes\n",
    "\n",
    "ex_audio, ex_label = train_dataset[random.randint(0,len(train_dataset))]\n",
    "\n",
    "ex_audio = ex_audio.unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "test_model = AudioCNN(input_size=input_size, num_classes=6)\n",
    "test_output = test_model(ex_audio)\n",
    "\n",
    "print(test_output)\n",
    "print(ex_label)\n",
    "_,predicted = torch.max(test_output,0)\n",
    "print(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEFORE GRADIENT STEP:\n",
      "prediction: tensor([[0.0121, 0.0000, 0.0937, 0.0000, 0.0454, 0.0000]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "label: tensor([0., 1., 0., 0., 0., 0.])\n",
      "loss tensor(1.8176, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "AFTER GRADIENT STEP:\n",
      "prediction: tensor([[0.0078, 0.0000, 0.0871, 0.0000, 0.0413, 0.0000]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "label: tensor([1])\n",
      "loss 1.8149813413619995\n",
      "\n",
      "Difference in loss: tensor(0.0026, grad_fn=<SubBackward0>)\n",
      "This should be some positive number to say we reduced loss\n"
     ]
    }
   ],
   "source": [
    "## Fill in the loss_function and optimizer below and run this cell to see if they are valid!\n",
    "\n",
    "model = AudioCNN(input_size=input_size, num_classes=6)\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()                      ## You should use CrossEntropyLoss, use the API to decide how to define this \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-5)      ## You can use SGD for this, which is defined in torch.optim -- look up some API stuff\n",
    "\n",
    "#############################################\n",
    "\n",
    "ex_audio, ex_label = train_dataset[random.randint(0,len(train_dataset))]\n",
    "\n",
    "ex_audio = ex_audio.unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "\n",
    "\n",
    "# This checks that your model, loss and optimizer are valid -- checkout what they print!\n",
    "print(\"BEFORE GRADIENT STEP:\")\n",
    "ex_pred = model(ex_audio) \n",
    "print(\"prediction:\",ex_pred)\n",
    "print(\"label:\",ex_label)\n",
    "\n",
    "\n",
    "optimizer.zero_grad() # Sets the gradient to 0 so that gradients don't stack together\n",
    "\n",
    "ex_label = ex_label.argmax().unsqueeze(0)\n",
    "\n",
    "ex_loss1 = loss_function(ex_pred, ex_label)\n",
    "print(\"loss\",ex_loss1)\n",
    "\n",
    "ex_loss1.backward() # This gets the gradient of the loss function w.r.t all of your model's params\n",
    "\n",
    "print()\n",
    "print(\"AFTER GRADIENT STEP:\")\n",
    "optimizer.step() # This takes the step to train\n",
    "\n",
    "ex_pred = model(ex_audio)\n",
    "print(\"prediction:\",ex_pred)\n",
    "print(\"label:\",ex_label)\n",
    "\n",
    "ex_loss2 = loss_function(ex_pred, ex_label)\n",
    "print(\"loss\",ex_loss2.item())\n",
    "\n",
    "print()\n",
    "print(\"Difference in loss:\", (ex_loss1 - ex_loss2))\n",
    "print(\"This should be some positive number to say we reduced loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training function here\n",
    "\n",
    "def training(model, loss_function, optimizer, train_dataloader, n_epochs, update_interval):\n",
    "    \n",
    "    losses = []\n",
    "\n",
    "    for n in range(n_epochs):\n",
    "        for i, (audio, label) in enumerate(tqdm(iter(train_dataloader))):\n",
    "\n",
    "            # TODO Complete the training loop using the instructions above\n",
    "            # Hint: the above code essentially does one training step\n",
    "\n",
    "            ##############################################################\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            audio = audio.unsqueeze(1)\n",
    "            pred = model(audio)\n",
    "            loss = loss_function(pred, label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            ##############################################################\n",
    "        \n",
    "            if i % update_interval == 0:\n",
    "                losses.append(round(loss.item(), 2)) # This will append your losses for plotting -- please use \"loss\" as the name for your loss\n",
    "        \n",
    "    return model, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 187/187 [10:24<00:00,  3.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg loss: 1.790526315789473\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plug in your model, loss function, and optimizer \n",
    "# Try out different hyperparameters and different models to see how they perform\n",
    "#train and tune\n",
    "\n",
    "lr = 1e-4               # The size of the step taken when doing gradient descent\n",
    "batch_size = 128       # The number of images being trained on at once\n",
    "update_interval = 10   # The number of batches trained on before recording loss\n",
    "n_epochs = 1            # The number of times we train through the entire dataset\n",
    "\n",
    "input_size = 80080 #no change\n",
    "num_classes = 6  # 6 emotion classes - no change\n",
    "\n",
    "train_dataset, test_dataset, train_dataloader, test_dataloader = load_dataset(crema_d_directory, batch_size=32)\n",
    "\n",
    "model = AudioCNN(input_size=input_size, num_classes=num_classes)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "trained_model, losses = training(model, loss_function, optimizer, train_dataloader, n_epochs=n_epochs, update_interval=update_interval)\n",
    " \n",
    "print(\"avg loss:\", sum(losses)/len(losses))\n",
    "\n",
    "plt.plot(np.arange(len(losses)) * batch_size * update_interval, losses)\n",
    "plt.title(\"training curve\")\n",
    "plt.xlabel(\"number of images trained on\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
