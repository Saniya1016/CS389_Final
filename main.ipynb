{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "import IPython.display as ipd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, Subset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision.utils import make_grid \n",
    "import matplotlib.pyplot as plt         \n",
    "from matplotlib.image import imread      \n",
    "import random\n",
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(directory, batch_size, train_ratio=0.8):\n",
    "    dataset = []\n",
    "    emotion_to_number = {'NEU': 0, 'HAP': 1, 'SAD': 2, 'ANG': 3, 'DIS': 4, 'FEA': 5}\n",
    "    max_len = 0\n",
    "    \n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.wav'):\n",
    "            filepath = os.path.join(directory, filename)\n",
    "            emotion = filename.split('_')[2]\n",
    "            emotion_no = emotion_to_number[emotion]\n",
    "            emotion_label = torch.zeros(6)\n",
    "            emotion_label[emotion_no] = 1\n",
    "            audio_tensor, _ = librosa.load(filepath, sr=None)\n",
    "            max_len = max(max_len, len(audio_tensor))\n",
    "            audio_tensor = torch.tensor(audio_tensor, dtype=torch.float32)\n",
    "            dataset.append([audio_tensor, emotion_label])\n",
    "\n",
    "    dataset = [(torch.nn.functional.pad(audio_tensor, (0, max_len - audio_tensor.size(0))), label)\n",
    "               for audio_tensor, label in dataset]\n",
    "\n",
    "    # Split the dataset into training and testing sets\n",
    "    train_size = int(train_ratio * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "    train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "    # Create DataLoader objects for training and testing sets\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return train_dataset, test_dataset, train_dataloader, test_dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0042, -0.0032, -0.0035,  ...,  0.0000,  0.0000,  0.0000])\n",
      "tensor([0., 0., 0., 1., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "crema_d_directory = \"./AudioWAV\"\n",
    "train_dataset, test_dataset, train_dataloader, test_dataloader = load_dataset(crema_d_directory, batch_size=32)\n",
    "ex_audio, ex_label = train_dataset[random.randint(0,20)]\n",
    "print(ex_audio)\n",
    "print(ex_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AudioCNN(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(AudioCNN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=32, kernel_size=3)\n",
    "        self.conv2 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3)\n",
    "        \n",
    "        # Calculate the output size of the convolutional layers\n",
    "        conv_output_size = (((input_size-2)//2) - 2)//2  # Considering two max pooling layers with kernel_size=2\n",
    "        \n",
    "        self.fc1 = nn.Linear(64 * conv_output_size, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool1d(x, kernel_size=2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool1d(x, kernel_size=2)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0032, 0.0000, 0.0039, 0.0369, 0.0215, 0.0083]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([0., 0., 0., 0., 0., 1.])\n",
      "tensor([0, 0, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_size = 80080\n",
    "num_classes = 6  # 6 emotion classes\n",
    "\n",
    "ex_audio, ex_label = train_dataset[random.randint(0,len(train_dataset))]\n",
    "\n",
    "ex_audio = ex_audio.unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "test_model = AudioCNN(input_size=input_size, num_classes=6)\n",
    "test_output = test_model(ex_audio)\n",
    "\n",
    "print(test_output)\n",
    "print(ex_label)\n",
    "_,predicted = torch.max(test_output,0)\n",
    "print(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEFORE GRADIENT STEP:\n",
      "prediction: tensor([[0.0000, 0.0000, 0.0617, 0.0000, 0.1284, 0.0000]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "label: tensor([0., 0., 0., 1., 0., 0.])\n",
      "loss tensor(1.8247, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "AFTER GRADIENT STEP:\n",
      "prediction: tensor([[0.0000, 0.0000, 0.0511, 0.0000, 0.1174, 0.0000]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "label: tensor([3])\n",
      "loss 1.8208308219909668\n",
      "\n",
      "Difference in loss: tensor(0.0038, grad_fn=<SubBackward0>)\n",
      "This should be some positive number to say we reduced loss\n"
     ]
    }
   ],
   "source": [
    "## Fill in the loss_function and optimizer below and run this cell to see if they are valid!\n",
    "\n",
    "model = AudioCNN(input_size=input_size, num_classes=6)\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()                      ## You should use CrossEntropyLoss, use the API to decide how to define this \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-5)      ## You can use SGD for this, which is defined in torch.optim -- look up some API stuff\n",
    "\n",
    "#############################################\n",
    "\n",
    "ex_audio, ex_label = train_dataset[random.randint(0,len(train_dataset))]\n",
    "\n",
    "ex_audio = ex_audio.unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "\n",
    "\n",
    "# This checks that your model, loss and optimizer are valid -- checkout what they print!\n",
    "print(\"BEFORE GRADIENT STEP:\")\n",
    "ex_pred = model(ex_audio) \n",
    "print(\"prediction:\",ex_pred)\n",
    "print(\"label:\",ex_label)\n",
    "\n",
    "\n",
    "optimizer.zero_grad() # Sets the gradient to 0 so that gradients don't stack together\n",
    "\n",
    "ex_label = ex_label.argmax().unsqueeze(0)\n",
    "\n",
    "ex_loss1 = loss_function(ex_pred, ex_label)\n",
    "print(\"loss\",ex_loss1)\n",
    "\n",
    "ex_loss1.backward() # This gets the gradient of the loss function w.r.t all of your model's params\n",
    "\n",
    "print()\n",
    "print(\"AFTER GRADIENT STEP:\")\n",
    "optimizer.step() # This takes the step to train\n",
    "\n",
    "ex_pred = model(ex_audio)\n",
    "print(\"prediction:\",ex_pred)\n",
    "print(\"label:\",ex_label)\n",
    "\n",
    "ex_loss2 = loss_function(ex_pred, ex_label)\n",
    "print(\"loss\",ex_loss2.item())\n",
    "\n",
    "print()\n",
    "print(\"Difference in loss:\", (ex_loss1 - ex_loss2))\n",
    "print(\"This should be some positive number to say we reduced loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training function here\n",
    "\n",
    "def training(model, loss_function, optimizer, train_dataloader, n_epochs, update_interval):\n",
    "    \n",
    "    losses = []\n",
    "\n",
    "    for n in range(n_epochs):\n",
    "        for i, (audio, label) in enumerate(tqdm(iter(train_dataloader))):\n",
    "\n",
    "            # TODO Complete the training loop using the instructions above\n",
    "            # Hint: the above code essentially does one training step\n",
    "\n",
    "            ##############################################################\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            audio = audio.unsqueeze(1)\n",
    "            pred = model(audio)\n",
    "            loss = loss_function(pred, label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            ##############################################################\n",
    "        \n",
    "            if i % update_interval == 0:\n",
    "                losses.append(round(loss.item(), 2)) # This will append your losses for plotting -- please use \"loss\" as the name for your loss\n",
    "        \n",
    "    return model, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [10:14<00:00, 13.07s/it]\n",
      "100%|██████████| 47/47 [09:12<00:00, 11.76s/it]\n",
      "100%|██████████| 47/47 [09:20<00:00, 11.94s/it]\n",
      "100%|██████████| 47/47 [09:23<00:00, 11.98s/it]\n",
      "100%|██████████| 47/47 [09:28<00:00, 12.09s/it]\n",
      "100%|██████████| 47/47 [10:38<00:00, 13.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg loss: 1.7903333333333327\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plug in your model, loss function, and optimizer \n",
    "# Try out different hyperparameters and different models to see how they perform\n",
    "#train and tune\n",
    "\n",
    "lr = 1e-4               # The size of the step taken when doing gradient descent\n",
    "batch_size = 128       # The number of images being trained on at once\n",
    "update_interval = 10   # The number of batches trained on before recording loss\n",
    "n_epochs = 6            # The number of times we train through the entire dataset\n",
    "\n",
    "input_size = 80080 #no change\n",
    "num_classes = 6  # 6 emotion classes - no change\n",
    "\n",
    "train_dataset, test_dataset, train_dataloader, test_dataloader = load_dataset(crema_d_directory, batch_size=batch_size)\n",
    "\n",
    "model = AudioCNN(input_size=input_size, num_classes=num_classes)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "trained_model, losses = training(model, loss_function, optimizer, train_dataloader, n_epochs=n_epochs, update_interval=update_interval)\n",
    " \n",
    "print(\"avg loss:\", sum(losses)/len(losses))\n",
    "\n",
    "plt.plot(np.arange(len(losses)) * batch_size * update_interval, losses)\n",
    "plt.title(\"training curve\")\n",
    "plt.xlabel(\"number of images trained on\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_accuracy(model, loss_function, test_data):\n",
    "\n",
    "    '''\n",
    "    This function will test the given model on the given test_data\n",
    "    it will return the accuracy and the test loss (given by loss_function) \n",
    "    '''\n",
    "    \n",
    "    sum_loss = 0\n",
    "    n_correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for i, (audio, label) in enumerate(tqdm(iter(test_dataloader))):\n",
    "\n",
    "        # This is essentially exactly the same as the training loop \n",
    "        # without the, well, training, part (and we record the accuracy too)\n",
    "        audio = audio.unsqueeze(1)\n",
    "        pred = model(audio)\n",
    "        loss = loss_function(pred, label)\n",
    "        sum_loss += loss.item()\n",
    "\n",
    "        _, predicted = torch.max(pred,1)\n",
    "        l = torch.argmax(label).item()\n",
    "        n_correct += (predicted == l).sum()\n",
    "        total += label.size(0)\n",
    "    \n",
    "    test_acc = round(((n_correct / total).item() * 100), 2)\n",
    "    avg_loss = round(sum_loss / len(test_data), 2)\n",
    "\n",
    "    print(\"test accuracy:\", test_acc)\n",
    "    print(\"test loss:\", avg_loss )\n",
    "\n",
    "    return test_acc, avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [01:38<00:00,  8.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy: 25.79\n",
      "test loss: 1.79\n",
      "Testing accuracy of your first model: 25.79\n",
      "Average loss of your first model: 1.79\n"
     ]
    }
   ],
   "source": [
    "# To see how well your model is doing without hyperpameter tuning\n",
    "test_acc,avg_loss = test_accuracy(model, loss_function, test_dataloader)\n",
    "\n",
    "print(\"Testing accuracy of your first model:\", test_acc)\n",
    "print(\"Average loss of your first model:\", avg_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
