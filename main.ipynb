{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, Subset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision.utils import make_grid  # |   Utility stuff for plotting\n",
    "import matplotlib.pyplot as plt          # |  <- I use this one a lot for plotting, seaborn is a good alternative\n",
    "from matplotlib.image import imread      # |  it reads images... (png -> usable input (like a numpy array for ex))\n",
    "import os\n",
    "import random\n",
    "from tqdm import tqdm  # | This one is a cute one for making a loading bar, I like it and we'll use it here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the CREMA-D AudioWAV dataset and dataloader\n",
    "def load_dataset(directory, batch_size):\n",
    "\n",
    "    dataset = []\n",
    "    emotion_to_number = {'NEU': 0, 'HAP': 1, 'SAD': 2, 'ANG': 3, 'DIS': 4, 'FEA': 5}\n",
    "    max_len = 0\n",
    "    \n",
    "    for filename in os.listdir(directory):\n",
    "        if(filename.endswith('.wav')):\n",
    "            filepath  = os.path.join(directory, filename)\n",
    "            emotion = filename.split('_')[2]\n",
    "            emotion_label = emotion_to_number[emotion]\n",
    "            audio_tensor, _ = librosa.load(filepath, sr=None)\n",
    "            max_len = max(max_len, len(audio_tensor))\n",
    "            audio_tensor = torch.tensor(audio_tensor, dtype=torch.float32)\n",
    "            dataset.append([audio_tensor, emotion_label])\n",
    "\n",
    "    dataset = [(torch.nn.functional.pad(audio_tensor, (0, max_len - audio_tensor.size(0))), label)\n",
    "                      for audio_tensor, label in dataset]\n",
    "\n",
    "    dataloader = torch.utils.data.DataLoader(dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True)\n",
    "            \n",
    "    return dataset, dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "crema_d_directory = \"./AudioWAV\"\n",
    "dataset, dataloader = load_dataset(crema_d_directory, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80080\n",
      "tensor([3, 3, 4, 3, 1, 1, 1, 3, 2, 0, 4, 2, 3, 2, 5, 0, 4, 4, 0, 3, 1, 3, 0, 0,\n",
      "        5, 0, 2, 0, 5, 5, 3, 2])\n"
     ]
    }
   ],
   "source": [
    "for inp, out in dataloader:\n",
    "    print(len(inp[0]))\n",
    "    print(out)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioCNN(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(AudioCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=32, kernel_size=3)\n",
    "        self.conv2 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3)\n",
    "        \n",
    "        # Calculate the output size of the convolutional layers\n",
    "        conv_output_size = (((input_size-2)//2) - 2)//2  # Considering two max pooling layers with kernel_size=2\n",
    "        \n",
    "        self.fc1 = nn.Linear(64 * conv_output_size, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    # def forward(self, x):\n",
    "    #     x = F.relu(self.conv1(x))\n",
    "    #     x = F.max_pool1d(x, kernel_size=2)\n",
    "    #     x = F.relu(self.conv2(x))\n",
    "    #     x = F.max_pool1d(x, kernel_size=2)\n",
    "    #     x = x.view(x.size(0), -1)\n",
    "    #     x = x.flatten()\n",
    "    #     x = F.relu(self.fc1(x))\n",
    "    #     x = F.relu(self.fc2(x))\n",
    "        \n",
    "    #     return xs\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool1d(x, kernel_size=2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool1d(x, kernel_size=2)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (64x20018 and 1281152x128)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-142-15c1e96a26b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mtest_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAudioCNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtest_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mex_audio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-141-be10e1065508>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (64x20018 and 1281152x128)"
     ]
    }
   ],
   "source": [
    "input_size = 80080\n",
    "num_classes = 6  # 6 emotion classes\n",
    "\n",
    "ex_audio, ex_label = dataset[random.randint(0,len(dataset))]\n",
    "ex_audio = ex_audio.unsqueeze(0)\n",
    "\n",
    "test_model = AudioCNN(input_size=input_size, num_classes=6)\n",
    "test_output = test_model(ex_audio)\n",
    "\n",
    "print(test_output)\n",
    "print(ex_label)\n",
    "_,predicted = torch.max(test_output,0)\n",
    "print(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training function here\n",
    "\n",
    "def training(model, loss_function, optimizer, train_dataloader, n_epochs, update_interval):\n",
    "    \n",
    "    losses = []\n",
    "\n",
    "    for n in range(n_epochs):\n",
    "        for i, (audio, label) in enumerate(tqdm(iter(train_dataloader))):\n",
    "\n",
    "            # TODO Complete the training loop using the instructions above\n",
    "            # Hint: the above code essentially does one training step\n",
    "\n",
    "            ##############################################################\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            audio = audio.unsqueeze(1) \n",
    "            pred = model(audio)\n",
    "            loss = loss_function(pred, label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            ##############################################################\n",
    "        \n",
    "            if i % update_interval == 0:\n",
    "                losses.append(round(loss.item(), 2)) # This will append your losses for plotting -- please use \"loss\" as the name for your loss\n",
    "        \n",
    "    return model, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 233/233 [14:37<00:00,  3.77s/it]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plug in your model, loss function, and optimizer \n",
    "# Try out different hyperparameters and different models to see how they perform\n",
    "\n",
    "lr = 0.001               # The size of the step taken when doing gradient descent\n",
    "batch_size = 32       # The number of images being trained on at once\n",
    "update_interval = 10   # The number of batches trained on before recording loss\n",
    "n_epochs = 1            # The number of times we train through the entire dataset\n",
    "input_size = 80080\n",
    "num_classes = 6  # 6 emotion classes\n",
    "\n",
    "train_dataset, train_dataloader = load_dataset(crema_d_directory, batch_size=batch_size)\n",
    "\n",
    "model = AudioCNN(input_size=input_size, num_classes=num_classes)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "trained_model, losses = training(model, loss_function, optimizer, train_dataloader, n_epochs=n_epochs, update_interval=update_interval)\n",
    "\n",
    "plt.plot(np.arange(len(losses)) * batch_size * update_interval, losses)\n",
    "plt.title(\"training curve\")\n",
    "plt.xlabel(\"number of images trained on\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
